### 注意力机制

引入：在复杂的环境中有效关注的点。

本质：考虑到我们的卷积全连接和池化的部分都不会考虑线索，不具备进行自主关注有效点。

通过注意力来有偏向性的选择某些输入。

注意力机制的重点就是让网络关注到他们所更需要关注的地方

？？？？？？？

感觉像是实现网络自适应的一种手段。

### 注意力机制的实现：

#### SENET：

$$
input=【24，24，32】\\
output=【24，24，64】\\
weight=【1，1，64】\\
chang=(24-24+0)/1+1=1\\
kuang=(24-24+0)/1+1=1\\
[1,1,64]-------->[1,1,32]---------->[1,1,64]
[1,1,64]*[24,24,64]
$$

通道注意力机制，这个实现的想法其实很简单，就是认为输入的通道上的特征是有重要性之分的，那么怎么能够让我们的网络更关注需要关注的通道上的信息呢，其实就是有没有可能在通道的featuremap上提取出一个数代表这个特征通道，然后对每个特征通道都进行提取后，最简单的关注方式：加权求和。这样就实现了对通道上信息的关注。

![image-20230422164900376](../Library/Application Support/typora-user-images/image-20230422164900376.png)

##### 实现的步骤：

```
1、对输入进来的特征层进行全局平均池化。
2、然后进行两次全连接，第一次全连接神经元个数较少，第二次全连接神经元个数和输入特征层相同。
3、在完成两次全连接后，我们再取一次Sigmoid将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。
4、在获得这个权值后，我们将这个权值乘上原输入特征层即可。
```

##### 实现的代码

```python
import torch
import torch.nn as nn
import math

class se_block(nn.Module):
    def __init__(self, channel, ratio=16):
        super(se_block, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        #自适应池化的操作，将其处理为一个关于C的一维度的向量
        self.fc = nn.Sequential(
                nn.Linear(channel, channel // ratio, bias=False),
                nn.ReLU(inplace=True),
                nn.Linear(channel // ratio, channel, bias=False),
                nn.Sigmoid()
        )
        #定义线性的层，先进行维度的缩减，再进行维度的增加，最后实现到我们的通道的数目上面，通过sigmoid函数其实就是在生成相应的权重的分数 。

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y
        #此处的返回的数值其实就是我们的原始的通道信息乘以我们处理后的个通道上面的权重
```

#### CMBAM：

CBAM其实是将通道注意力和空间注意力做了一个结合的操作，相比于SENET只关注我们的 通道注意力，其CBAM会对输入进来的特征层，分别进行通道注意力处理和空间注意力处理。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201124133821606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)

```python
下图是通道注意力机制和空间注意力机制的具体实现方式：
图像的上半部分为通道注意力机制，通道注意力机制的实现可以分为两个部分，我们会对输入进来的单个特征层，分别进行全局平均池化和全局最大池化。之后对平均池化和最大池化的结果，利用共享的全连接层进行处理，我们会对处理后的两个结果进行相加，然后取一个sigmoid，此时我们获得了输入特征层每一个通道的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。

图像的下半部分为空间注意力机制，我们会对输入进来的特征层，在每一个特征点的通道上取最大值和平均值。之后将这两个结果进行一个堆叠，利用一次通道数为1的卷积调整通道数，然后取一个sigmoid，此时我们获得了输入特征层每一个特征点的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201124134115869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)

```python
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        #通道注意力，这里主要是通过定义申明俩个最大池化和平均池化
        #利用1x1卷积代替全连接
        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class cbam_block(nn.Module):
    def __init__(self, channel, ratio=8, kernel_size=7):
        super(cbam_block, self).__init__()
        self.channelattention = ChannelAttention(channel, ratio=ratio)
        self.spatialattention = SpatialAttention(kernel_size=kernel_size)

    def forward(self, x):
        x = x * self.channelattention(x)
        x = x * self.spatialattention(x)
        return x

```

#### ECA 注意力：

ECANet是也是通道注意力机制的一种实现形式。ECANet可以看作是SENet的改进版。
ECANet的作者认为SENet对通道注意力机制的预测带来了副作用，捕获所有通道的依赖关系是低效并且是不必要的。
在ECANet的论文中，作者认为卷积具有良好的跨通道信息获取能力。

ECA模块的思想是非常简单的，它去除了原来SE模块中的全连接层，直接在全局平均池化之后的特征上通过一个1D卷积进行学习。

既然使用到了1D卷积，那么1D卷积的卷积核大小的选择就变得非常重要了，了解过卷积原理的同学很快就可以明白，1D卷积的卷积核大小会影响注意力机制每个权重的计算要考虑的通道数量。用更专业的名词就是跨通道交互的覆盖率。

如下图所示，左图是常规的SE模块，右图是ECA模块。ECA模块用1D卷积替换两次全连接。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20201126210628785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)

```python
class eca_block(nn.Module):
    def __init__(self, channel, b=1, gamma=2):
        super(eca_block, self).__init__()
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) 
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return x * y.expand_as(x)

```

##### Self—attention：

提出时候想解决的问题：

![image-20230422174142732](../Library/Application Support/typora-user-images/image-20230422174142732.png)

起源于语音信号的处理，

输入：一堆的向量的组合：

![image-20230422185820034](../Library/Application Support/typora-user-images/image-20230422185820034.png)

输出呢：：：：：：？？

第一种：每个输入的向量有一个label

![image-20230422190230185](../Library/Application Support/typora-user-images/image-20230422190230185.png)

第二种：整个的输入有一个label

![image-20230422190355464](../Library/Application Support/typora-user-images/image-20230422190355464.png)

第三种：不知道需要的输出的向量：

![image-20230422190445168](../Library/Application Support/typora-user-images/image-20230422190445168.png)

词性的标注：

![image-20230422190904595](../Library/Application Support/typora-user-images/image-20230422190904595.png)

有没有可能让我们的网络考虑到这个上下文，然后让网络去考判别：

![image-20230422191045712](../Library/Application Support/typora-user-images/image-20230422191045712.png)

但是有时候需要考虑整个sequence，但是window实现不了。

既然window实现不了，想办法：

怎么办？？

##### self_attention：

![image-20230422191236242](../Library/Application Support/typora-user-images/image-20230422191236242.png)

输入一整个sequence的信息，输出是一整个sequence的相互影响的结果，输入几个向量就输出几个向量，但是这个输出的向量考虑到了整个的sequence的

不在局限于考虑一个window的信息。

![image-20230422191521389](../Library/Application Support/typora-user-images/image-20230422191521389.png)

我们的self_attention可以多次使用。

文章的来源：attention is all you need

详解我们的self- attention：

![image-20230422191823311](../Library/Application Support/typora-user-images/image-20230422191823311.png)

具体的的输出的向量的输出的呢？

首先，我们的这个self_attenttion 就是实现我们的输入的一一整个之间的相关性，然后结合上下文推断出输出的向量。其实就是在利用全局的咨询来获取我们的单个输出。

![image-20230422192134868](../Library/Application Support/typora-user-images/image-20230422192134868.png)

计算attention的模组：

![image-20230422192303323](../Library/Application Support/typora-user-images/image-20230422192303323.png)

输入的两个向量，我们可以通过这个来计算两个向量之间的相关性：
$$
\alpha=q.k
$$
![image-20230422193125779](../Library/Application Support/typora-user-images/image-20230422193125779.png)

通过这种方法计算得到我们的相关性之间的得分

![image-20230422193240522](../Library/Application Support/typora-user-images/image-20230422193240522.png)

计算出所有的向量之间的关联性之后：

![image-20230422193309328](../Library/Application Support/typora-user-images/image-20230422193309328.png)

现在的这个a其实就是我们的各个部分的权重的得分的情况：

![image-20230422193446433](../Library/Application Support/typora-user-images/image-20230422193446433.png)

回想一下，我们在这里做这个的操作其实就是计算相邻的向量之间的关联性，然后怎么处理n？

计算出这个关联性之后就可以利用这个相关性之间的评价进行加权，进而输出得到我们的目的新的向量；

那这个新的向量中有什么呢？

其实我感觉 这个新的向量就是在考虑了全局的上下文的相关性之后，将其每个之间的相关性按照权重进行组合到新的输出中，这样的组合就实现了全局的信息的提取，野可以关注到重要的信息。

##### self_attention下：

self_attention的运作的过程：

![image-20230422194219231](../Library/Application Support/typora-user-images/image-20230422194219231.png)

矩阵的原理：

首先利用输入的矩阵通过线性变换出现Q
$$
q^1q^2q^3q^4=W^qa^1a^2a^3a^4\\
Q=W*I
$$

$$
k^1k^2k^3k^4=W^ka^1a^2a^3a^4\\
K=W*I
$$

$$
v^1v^2v^3v^4=W^va^1a^2a^3a^4\\
V=W*I
$$

![image-20230422195133457](../Library/Application Support/typora-user-images/image-20230422195133457.png)

![image-20230422195225848](../Library/Application Support/typora-user-images/image-20230422195225848.png)

已经计算出我们的各个向量之间的相关性得分的情况的矩阵：

![image-20230422195447847](../Library/Application Support/typora-user-images/image-20230422195447847.png)

![image-20230422195648856](../Library/Application Support/typora-user-images/image-20230422195648856.png)

#### multi-head self-attention：

多头自注意力机制：

为什么考虑去使用这个多头注意力机制呢？

就是因为在考虑一个全局的相关性的时候，我们考虑到可能这些的向量之间存在不同的相关性。

设置多个头让去捕获多种相关性：

![image-20230422200314375](../Library/Application Support/typora-user-images/image-20230422200314375.png)

![image-20230422200332309](../Library/Application Support/typora-user-images/image-20230422200332309.png)

将最终的结果通过一个变换得到最后的输出的bi

#### Positional Encoding:

位置编码：

位置信息在某一些的问题中比较重要的时候

![image-20230422200635159](../Library/Application Support/typora-user-images/image-20230422200635159.png)

让我们的网络获取到位置的信息：

##### 实际的应用：

语音辨识识别：

![image-20230422201432830](../Library/Application Support/typora-user-images/image-20230422201432830.png)

考虑一段的的自注意力；

图片的问题：

![image-20230422201621677](../Library/Application Support/typora-user-images/image-20230422201621677.png)

##### RNN的简述：

![image-20230426134611920](../Library/Application Support/typora-user-images/image-20230426134611920.png)



### Transformer

##### Sequence to sequence:

